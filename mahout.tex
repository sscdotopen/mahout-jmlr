\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage[utf8]{inputenc}
\usepackage{listings}
% Definitions of handy macros can go here

\usepackage[dvipsnames]{xcolor}
\usepackage{wrapfig}

\lstset{
  language=Scala,
  basicstyle=\footnotesize\ttfamily,
  commentstyle=\color{gray},   
  frame=tb,
  numbers=left,
  numberstyle=\tiny,
  emph={%  
    val, def%
    },emphstyle={\bfseries\color{MidnightBlue}}%  
}

\newcommand{\todo}[1]{\textcolor{purple}{TODO: #1}}
\newcommand{\dataset}{{\cal D}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2018}{1-48}{4/00}{10/00}{meila00a}{Anil et al.: Apache Mahout}

% Short headings should be running head and authors last names

\ShortHeadings{Apache Mahout: Machine Learning on Distributed Dataflow Systems}{Anil et al.}
\firstpageno{1}

\begin{document}

\title{Apache Mahout: Machine Learning on Distributed Dataflow Systems}

\author{\name Robin Anil \email robinanil@apache.org\\
  \addr Tock, Chicago, US\\
  %
  \name Gokhan Capan \email gcapan@apache.org\\
  \addr Persona.tech, Istanbul, Turkey\\
  %
  \name Isabel Drost-Fromm \email isabel@apache.org\\
  \addr Europace AG, Berlin, Germany\\
  %
  \name Ted Dunning \email tdunning@apache.org\\
  \name Ellen Friedman \email ellenf@apache.org\\
  \addr MapR Technologies, Santa Clara, US\\
  %
  \name Trevor Grant \email rawkintrevo@apache.org\\
  \addr IBM, Chicago, US\\
  %
  \name Shannon Quinn \email squinn@apache.org\\
  \addr Department of Computer Science, University of Georgia\\
  \addr Athens, US\\ 
  %
  \name Paritosh Ranjan \email pranjan@apache.org\\
  %
  \name Sebastian Schelter \email ssc@apache.org\\
  \addr Center for Data Science, New York University\\
  \addr New York, US\\
  %
  \name Özgür Yılmazel \email oyilmazel@apache.org\\
  \addr Anadolu University\\
  \addr Tepebaşı / Eskişehir, Turkey\\
}

\editor{}

\maketitle

\begin{abstract}%  
\textsc{Apache Mahout} is a library for scalable machine learning (ML) on distributed dataflow systems, offering various implementations of classification, clustering, dimensionality reduction and recommendation algorithms. It originated in 2008 and targeted MapReduce, which was the predominant abstraction for scalable computing in industry at that time. Mahout has been widely used by leading web companies and is part of commercial cloud offerings. In recent years, Mahout migrated to a general framework enabling a mix of dataflow programming and linear algebraic computations on backends such as \textsc{Apache Spark}, \textsc{Apache Flink} and \textsc{H20}. Mahout is maintained as a community-driven, top-level, open source project at the Apache Software Foundation, and is available under \url{https://mahout.apache.org}. 
\end{abstract}


\section{Introduction}

\textsc{Mahout} was started in 2008 as a subproject of the open source search engine \textit{Apache Lucene}~(\cite{Owen2012,Mccandless2010}), when the information retrieval community encountered a growing need for applying ML techniques on large text corpora. In 2010, Mahout became a top-level Apache project. At the time when Mahout emerged, \textit{Apache Hadoop} was the dominant platform for storing and processing large datasets, where data was persisted based on an open source implementation of the Google filesystem~(\cite{Ghemawat2003}) and processed using the MapReduce paradigm~(\cite{Dean2008}), which was initially developed for building the search index of webscale search engines. Due to the prevalence of Hadoop in industry, as well as research which indicated that a large family of popular ML algorithms can be reformulated under the MapReduce paradigm~(\cite{Chu2007}), Mahout initially focused on MapReduce-based algorithm implementations. These implementations have been widely used industry, including by leading web companies\footnote{\url{https://mahout.apache.org/general/powered-by-mahout.html}} such as Twitter, Linkedin and Foursquare, and are available in major commercial cloud offerings such as Amazon's \textit{Elastic MapReduce} service\footnote{\url{https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-mahout.html}} and Microsoft's \textit{Azure HDInsight}\footnote{\url{https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-component-versioning}}.

While data is still stored in the Hadoop filesystem in many industry deployments, the actual platforms and paradigms to process this data have changed tremendously, mainly due to performance and usability problems with MapReduce. Current paradigms and systems range from analytical databases, to general dataflow engines, to specialized machine learning systems. Therefore, Mahout has evolved to leverage a domain-specific language (DSL) called \textsc{Samsara} for current algorithm implementations, which can be executed on a variety of different platforms. In the remainder of this paper, we will first introduce Mahout's `legacy' algorithms implemented on MapReduce in Section\ref{sec:legacy}, and afterwards describe the Samsara language in Section~\ref{sec:samsara}.

\section{Legacy: MapReduce-based Algorithms}
\label{sec:legacy}

\textit{Collaborative Filtering}. Mahout features a huge variety of collaborative filtering algorithms for recommendation scenarios. A simple, well-working and widely deployed nearest-neighbor-based approach is item-based collaborative filtering~(\cite{Sarwar2001}), where a matrix of similarities between the interaction vectors of all item pairs is computed, and leveraged to derive recommendations later on. Mahout features various implementations of this approach, both distributed and non-distributed~(\cite{,Dunning1993,Schelter2012,Dunning2014}). Another popular technique to analyze interactions between users and items are so-called latent factor models~(\cite{Koren2009}). They factor a sparse partially-observed user-item interaction matrix into the product of two low-rank matrices, such that their that their product approximates the observed parts of the interaction matrix and generalizes well to unobserved parts of the same. Analogous to the neighborhood-based methods, Mahout provides distributed and non-distributed implementations of latent factor models as well. It contains stochastic gradient descent (SGD)-based learners (including Hogwild!-style parallelization~(\cite{Bennett2007,Recht2011})), as well as different variants of alternating-least-squares-based approaches~(\cite{Zhou2008,Hu2008,Schelter2013}).\\

\textit{Classification}. Mahout contains a distributed implementation of Naive Bayes with preprocessing steps tailored for textual data~(\cite{Rennie2003}). Naive Bayes fits the MapReduce paradigm particularly well as it only requires a small, fixed number of passes over the data, which compute aggregates and are easy to parallelize. Additionally, Mahout features a single machine implementation of logistic regression learned with SGD, which allows its users to train models in an incremental fashion. The SGD framework includes an online evaluation component using cross-validation, which runs several learners in separate threads with different hyperparameter settings. This implementation also includes a library which allows users to easily encode different types of features. Furthermore, Mahout contains a MapReduce-based implementation of Random Forests~(\cite{Breiman2001}), and a single-machine implementation for learning Hidden Markov Models.\\

\textit{Clustering}. Mahout includes MapReduce-based implementations of $k$-Means clustering and canopy clustering~(\cite{Mccallum2000}). $k$-means is easy to parallelize as the distance computation between centroids and data points is embarassingly parallel, and the re-computation of the centroids can be executed using a single distributed aggregation. Additionally, Mahout includes a streaming version of $k$-Means. This approach first conducts a streaming pass through the data and produces a large number of temporary centroids, after which a `ball $k$-Means' step~(\cite{Shindler2011,Ostrovsky2012}) will further reduce the number of clusters down to $k$.\\

\textit{Dimensionality Reduction}. Mahout contains implementations of two algorithms to compute the singular value decomposition (SVD) of large matrices: MapReduce-based versions of the Lanczos algorithm~(\cite{Golub2012}), which conducts a series of distributed matrix vector multiplications, and of Stochastic SVD~(\cite{Halko2012}) which only requires a fixed number of passes over the data. Furthermore, Mahout features MapReduce-based implementations for computing embeddings of textual data such as Latent Semantic Analysis~(\cite{Deerwester1990}) and Latent Dirichlet Allocation~(\cite{Blei2003}).



\section{Mahout Samsara}
\label{sec:samsara}

As already mentioned in the introduction, it became clear over time that the MapReduce paradigm is suboptimal for the distributed execution of ML algorithms, both for reasons of usability and performance. At the same time, the underlying Hadoop platform has been rewritten to expose resource management and job scheduling capabilities\footnote{\url{https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html}} to allow systems with parallel processing paradigms different from MapReduce to operate on data stored in the distributed filesystem. Examples of such systems are Apache~Spark~(\cite{Zaharia2012}), Apache~Flink~(\cite{Alexandrov2014}) and H2o~(\cite{H2O}). 

Unfortunately, these systems are still difficult to program, as their programming model is heavily influenced by the underlying data-parallel execution scheme. Usually, programs consist of a sequence of parallelizable second-order functions (such as \texttt{map}, \texttt{reduce} or \texttt{groupBy}) that dictate how the system should execute user-defined first-order functions on partitioned data. Such programming models are non-intuitive for users without a background in distributed systems, and are in general hard to program without a detailed understanding of the underlying execution model. Furthermore, the available programming abstractions typically rely on partitioned, unordered sets; this is a mismatch for ML applications that mostly operate on matrices and vectors. Therefore, implementing ML algorithms on dataflow systems is a tedious and difficult task. 

As a consequence, Mahout has been rebuilt on top of \textsc{Samsara}~(\cite{Lyubimov2016}), a domain-specific language for declarative machine learning in cluster environments. Samsara allows its users to specify programs using a set of common matrix abstractions and linear algebraic operations, similar to \textsc{R} or \textsc{MATLAB}. Samsara then compiles, optimizes and executes these programs on distributed dataflow systems~(\cite{Schelter2016}). The aim of Samsara is to allow mathematicians and data scientists to leverage the scalability of distributed dataflow systems via common declarative abstractions, while drastically reducing the need for detailed knowledge of the programming model and execution scheme of the underlying systems.
%
\begin{wrapfigure}{r}{0.4\textwidth}
  \begin{center}
    \includegraphics[scale=0.33]{figures/architecture-crop}
    \caption{Samsara architecture.\label{fig:architecture}}
  \end{center}
\end{wrapfigure}
%

Figure~\ref{fig:architecture} illustrates the architecture of Samsara. Applications are written using the Scala DSL, and developers have to choose between an in-memory and a distributed representation of matrices used in the program. Operations on in-memory matrices are immediately executed, while operations on distributed matrices (which are partitioned among the machines in the cluster) are deferred. The system records the actions to perform on these distributed matrices, and internally builds a directed acyclic graph (DAG) of logical operations from them, where vertices refer to matrices and edges correspond to transformations between them. Materialization barriers (e.g., persisting a result or collecting a matrix into local memory) implicitly trigger execution. Upon execution, the DAG of logical operators is optimized, e.g., by removing redundant transpose operations and by choosing execution strategies for matrix multiplications based on the shape of the operands. The program is then transformed into a DAG of physical operators to execute, which are specific to one of the backends that Samsara supports (currently Apache~Spark, Apache~Flink and H20), and its distributed parts are executed by the respective backend. A current effort is to support the native execution of costly matrix operations on GPUs via an integration of the ViennaCL~(\cite{Rupp2010}) framework.

\section{Availability and Requirements}

Mahout is run as a top-level project under the umbrella of the Apache Software Foundation, and developed in a community-driven, meritocratic fashion according to the \textit{Apache~Way}\footnote{\url{https://www.apache.org/foundation/how-it-works.html}}. Mahout is available under the Apache License at \url{https://mahout.apache.org}. The latest version v0.13.0 requires at least Java~7 and Scala~2.10 for Samsara. The legacy algorithms require Hadoop~2.4, while Samsara programs can be executed on Flink~1.1, Spark~1.6/2.x and H20~0.1.25.

\acks{}
Over the years, Mahout has been the product of the efforts of numerous people. The authors of this paper we would like to thank: Abdelhakim Deneche, Anand Avati, Andrew Musselman, Benson Margulies, Dan Filimon, David Hall, Dawid Weiss, Dmitriy Lyubimov, Drew Farris, Erik Hatcher, Frank Scholten, Grant Ingersoll, Jake Mannix, Jeff Eastman, Karl Wettin, Niranjan Balasubramanian, Otis Gospodnetic, Pat Ferrel, Sean Owen, Stevo Slavić, Suneel Marthi and Tom Pierce. Of course we extend our thanks to all users and contributors as well.

\bibliography{mahout}

\end{document}
 